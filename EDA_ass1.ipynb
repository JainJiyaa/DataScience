{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef69611a-d95e-401a-a8f0-d248876dc418",
   "metadata": {},
   "source": [
    "Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in\n",
    "predicting the quality of wine.\n",
    "\n",
    "Some of the key features of wine quality dataset are as follows:\n",
    "Fixed Acidity.\n",
    "Volatile Acidity.\n",
    "Citric Acid.\n",
    "Residual Sugar.\n",
    "Chlorides.\n",
    "Free Sulfur Dioxide.\n",
    "Total Sulfur Dioxide.\n",
    "Density.\n",
    "pH.\n",
    "Sulphates.\n",
    "Alcohol.\n",
    "Quality.\n",
    "\n",
    "IMPORTANCE OF EACH FEATURE:\n",
    "Fixed Acidity: Impacts the sharpness and tartness of the wine. Essential for balance and complexity.\n",
    "Volatile Acidity: Crucial for quality control. High levels indicate spoilage or poor fermentation practices.\n",
    "Citric Acid: Adds freshness and enhances flavor. Helps in preventing spoilage and microbial growth.\n",
    "Residual Sugar: Balances acidity and influences the wine's sweetness profile. Important for consumer preference.\n",
    "Chlorides: Reflects on the mineral content and can influence the taste negatively if high.\n",
    "Free Sulfur Dioxide: Essential for preventing oxidation and microbial spoilage without affecting taste significantly.\n",
    "Total Sulfur Dioxide: High levels can result in unpleasant aromas and flavors. Balance is crucial.\n",
    "Density: Indicates the wine's richness and potential sweetness/alcohol content. Important for texture and mouthfeel.\n",
    "pH: Affects the stability, color, and taste of wine. Essential for microbial stability and aging potential.\n",
    "Sulphates: Acts as an antioxidant and influences the preservation quality. Needs to be balanced for optimal flavor.\n",
    "Alcohol: Directly affects body, taste, and warmth. Higher alcohol content usually correlates with higher quality perception."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b60dbcf-56db-4d40-a208-858a9405b7da",
   "metadata": {},
   "source": [
    "Q2. How did you handle missing data in the wine quality data set during the feature engineering process?\n",
    "Discuss the advantages and disadvantages of different imputation techniques.\n",
    "\n",
    "1.Removing Rows/Columns with Missing Data.\n",
    "Advantages:\n",
    "Simple and straightforward.\n",
    "No need for complex computations.\n",
    "Disadvantages:\n",
    "Loss of data can reduce the dataset size, leading to potential loss of valuable information.\n",
    "Not suitable if a large portion of the data is missing.\n",
    "\n",
    "2.Mean and Median Imputation.\n",
    "Advantages:\n",
    "Simple to implement.\n",
    "Maintains the dataset size.\n",
    "Disadvantages:\n",
    "Can distort the distribution of the data, especially if the data is not normally distributed.\n",
    "Ignores the relationships between features.\n",
    "\n",
    "3.Mode Imputation.\n",
    "Advantages:\n",
    "Simple and effective for categorical data.\n",
    "Maintains the dataset size.\n",
    "Disadvantages:\n",
    "Can introduce bias, especially if the mode is not representative of the missing values.\n",
    "Less useful for numerical data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac50557-ad47-453f-ba03-618bee0f0872",
   "metadata": {},
   "source": [
    "Q3. What are the key factors that affect students' performance in exams? How would you go about\n",
    "analyzing these factors using statistical techniques?\n",
    "\n",
    "*Key factors that affect students' performance in exams can include:\n",
    "\n",
    "Demographic Factors.\n",
    "Academic Background.\n",
    "Study Habits.\n",
    "Psychological Factors.\n",
    "Environmental Factors.\n",
    "Health Factors.\n",
    "\n",
    "*Analyzing These Factors Using Statistical Techniques:\n",
    "\n",
    "Data Collection: Gather data from surveys, academic records, and other relevant sources.\n",
    "\n",
    "Descriptive Statistics:\n",
    "Mean, Median, Mode: Summarize central tendencies of each factor.\n",
    "Standard Deviation, Variance: Assess the variability in the data.\n",
    "Frequency Distribution: Understand the distribution of categorical variables.\n",
    "\n",
    "Visualization:\n",
    "Histograms: Visualize the distribution of continuous variables.\n",
    "Box Plots: Identify outliers and compare distributions.\n",
    "Scatter Plots: Examine relationships between continuous variables.\n",
    "Bar Charts: Compare categorical data.\n",
    "\n",
    "Correlation Analysis:\n",
    "Pearson/Spearman Correlation Coefficients: Measure the strength and direction of relationships between continuous variables.\n",
    "Heatmaps: Visualize the correlation matrix.\n",
    "\n",
    "Hypothesis Testing:\n",
    "T-tests/ANOVA: Compare means between different groups (e.g., gender, study habits).\n",
    "Chi-square Tests: Assess relationships between categorical variables.\n",
    "\n",
    "Regression Analysis:\n",
    "Linear Regression: Predict exam scores based on continuous predictor variables.\n",
    "Logistic Regression: If predicting a binary outcome (e.g., pass/fail).\n",
    "Multiple Regression: Include multiple predictors to understand their combined effect on exam performance.\n",
    "\n",
    "Feature Importance:\n",
    "Decision Trees/Random Forests: Identify the most important features influencing exam performance.\n",
    "Feature Selection Techniques: Use methods like forward selection, backward elimination, or regularization techniques (Lasso, Ridge).\n",
    "Clustering:\n",
    "\n",
    "K-Means/Hierarchical Clustering: Group students with similar characteristics to identify common patterns in performance.\n",
    "\n",
    "Principal Component Analysis (PCA):\n",
    "Dimensionality Reduction: Identify key factors by reducing the dataset's dimensionality while retaining variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d428d5b-ef13-4fb9-915e-37fad698f200",
   "metadata": {},
   "source": [
    "Q4. Describe the process of feature engineering in the context of the student performance data set. How\n",
    "did you select and transform the variables for your model?\n",
    "\n",
    "Feature engineering in the context of the student performance dataset involves selecting, creating, and transforming variables to improve the predictive power of the model.\n",
    "Through feature engineering, we select and transform variables to enhance the modelâ€™s ability to predict student performance. This process involves understanding the data, cleaning it, selecting significant features, transforming variables, creating new features, and using model-based techniques to identify the most impactful features. This iterative process improves the accuracy and robustness of predictive models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b850e95-2130-435c-8273-a8b904881c49",
   "metadata": {},
   "source": [
    "Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distribution\n",
    "of each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to\n",
    "these features to improve normality?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170b16ed-d803-4ede-a6c2-ade50a1972d2",
   "metadata": {},
   "source": [
    "To perform exploratory data analysis (EDA) on the wine quality dataset and identify the distribution of each feature, follow these steps:\n",
    "\n",
    "Load the Dataset:\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('winequality-red.csv')  # Adjust the path if necessary\n",
    "Examine the Distribution of Each Feature:\n",
    "# Plot histograms for each feature\n",
    "data.hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "Check for Non-Normality:\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "# Function to check normality\n",
    "def check_normality(data):\n",
    "    normality_results = {}\n",
    "    for column in data.columns:\n",
    "        stat, p = shapiro(data[column])\n",
    "        normality_results[column] = p\n",
    "    return normality_results\n",
    "\n",
    "normality_results = check_normality(data)\n",
    "non_normal_features = [feature for feature, p in normality_results.items() if p < 0.05]\n",
    "print(\"Non-normal features:\", non_normal_features)\n",
    "Transformations to Improve Normality:\n",
    "\n",
    "Log Transformation: Apply to features with positive values only.\n",
    "Square Root Transformation: Useful for reducing skewness.\n",
    "Box-Cox Transformation: Applies to positive data, more flexible.\n",
    "Yeo-Johnson Transformation: Works for both positive and negative values.\n",
    "Apply Transformations:\n",
    "import numpy as np\n",
    "from scipy.stats import boxcox, yeojohnson\n",
    "\n",
    "# Log transformation (example)\n",
    "data['fixed_acidity_log'] = np.log1p(data['fixed_acidity'])\n",
    "\n",
    "# Square root transformation (example)\n",
    "data['fixed_acidity_sqrt'] = np.sqrt(data['fixed_acidity'])\n",
    "\n",
    "# Box-Cox transformation (example)\n",
    "data['fixed_acidity_boxcox'], _ = boxcox(data['fixed_acidity'] + 1)  # Adding 1 to avoid log(0)\n",
    "\n",
    "# Yeo-Johnson transformation (example)\n",
    "data['fixed_acidity_yeojohnson'], _ = yeojohnson(data['fixed_acidity'])\n",
    "Summary of EDA Steps:\n",
    "Load and Examine Data: Load the dataset and plot histograms to visualize feature distributions.\n",
    "Check Normality: Use statistical tests (e.g., Shapiro-Wilk) to identify non-normal features.\n",
    "Transform Features: Apply transformations like log, square root, Box-Cox, or Yeo-Johnson to improve normality.\n",
    "Example Code:\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import shapiro, boxcox, yeojohnson\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('winequality-red.csv')\n",
    "\n",
    "# Plot histograms for each feature\n",
    "data.hist(bins=30, figsize=(15, 10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Check normality\n",
    "def check_normality(data):\n",
    "    normality_results = {}\n",
    "    for column in data.columns:\n",
    "        stat, p = shapiro(data[column])\n",
    "        normality_results[column] = p\n",
    "    return normality_results\n",
    "\n",
    "normality_results = check_normality(data)\n",
    "non_normal_features = [feature for feature, p in normality_results.items() if p < 0.05]\n",
    "print(\"Non-normal features:\", non_normal_features)\n",
    "\n",
    "# Apply transformations\n",
    "for feature in non_normal_features:\n",
    "    data[feature + '_log'] = np.log1p(data[feature])\n",
    "    data[feature + '_sqrt'] = np.sqrt(data[feature])\n",
    "    data[feature + '_boxcox'], _ = boxcox(data[feature] + 1)\n",
    "    data[feature + '_yeojohnson'], _ = yeojohnson(data[feature])\n",
    "\n",
    "# Plot histograms for transformed features\n",
    "transformed_features = [feature + '_log' for feature in non_normal_features] + \\\n",
    "                       [feature + '_sqrt' for feature in non_normal_features] + \\\n",
    "                       [feature + '_boxcox' for feature in non_normal_features] + \\\n",
    "                       [feature + '_yeojohnson' for feature in non_normal_features]\n",
    "\n",
    "data[transformed_features].hist(bins=30, figsize=(20, 15))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "This code provides a comprehensive approach to perform EDA, identify non-normal features, and apply transformations to improve normality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ad09ba-0ff9-410a-8c7c-3b7b2819b42b",
   "metadata": {},
   "source": [
    "Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of\n",
    "features. What is the minimum number of principal components required to explain 90% of the variance in\n",
    "the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ee425-2c7b-4aff-a791-29bb1a77a0ca",
   "metadata": {},
   "source": [
    "To perform Principal Component Analysis (PCA) on the wine quality dataset and determine the minimum number of principal components required to explain 90% of the variance, follow these steps:\n",
    "\n",
    "Load the Dataset:\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('winequality-red.csv')\n",
    "features = data.drop('quality', axis=1)  # Exclude the target variable if present\n",
    "\n",
    "Standardize the Data:\n",
    "scaler = StandardScaler()\n",
    "scaled_features = scaler.fit_transform(features)\n",
    "\n",
    "Perform PCA:\n",
    "pca = PCA()\n",
    "pca.fit(scaled_features)\n",
    "Determine the Number of Components to Explain 90% Variance:\n",
    "cumulative_variance = pca.explained_variance_ratio_.cumsum()\n",
    "num_components = next(i for i, cumulative_var in enumerate(cumulative_variance) if cumulative_var >= 0.90) + 1\n",
    "print(f\"Number of components to explain 90% variance: {num_components}\")\n",
    "Example Code:\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
