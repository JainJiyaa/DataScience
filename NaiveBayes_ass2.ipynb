{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e57d05a4-e772-4759-b839-bc76fb4bc51d",
   "metadata": {},
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "The probability that an employee is a smoker given that he/she uses the health insurance plan is 40%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d42a187b-e57f-433b-9932-27c45cdfad69",
   "metadata": {},
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Bernoulli Naive Bayes is used for binary/boolean features, where each feature represents the presence or absence of something (e.g., a word in a document). Multinomial Naive Bayes is used for discrete, count-based features, typically representing the frequency of occurrences (e.g., the number of times a word appears in a document)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce418851-4354-4325-87a0-d0a713fcda15",
   "metadata": {},
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "Bernoulli Naive Bayes treats missing values as absent (i.e., zero or false), assuming the feature is not present."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eacf262-63a8-4789-9740-e1602ef19d29",
   "metadata": {},
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "Yes, Gaussian Naive Bayes can be used for multi-class classification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b9c130-8aa1-4bc0-aeaa-179d0a51d815",
   "metadata": {},
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c28a53d1-b4ad-4078-8828-0497591f6d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Discussion:\\nBest Performing Variant: MultinomialNB generally performs the best on text data because it handles frequency counts of words efficiently, which is typical in spam detection tasks.\\nLimitations:\\nNaive Bayes assumes feature independence, which might not hold in real-world data.\\nGaussianNB may not perform well on data that is not normally distributed.\\nConclusion:\\nSummary: MultinomialNB outperformed BernoulliNB and GaussianNB due to its suitability for count-based data in text classification.\\nFuture Work:\\nExperiment with feature engineering and hyperparameter tuning.\\nCompare Naive Bayes with other advanced classifiers like SVM or Random Forest.\\nThis approach provides a comprehensive yet concise analysis of the Naive Bayes variants on the Spambase dataset'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Assignment\n",
    "Data Preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository.\n",
    "\n",
    "Implementation:\n",
    "Load Data:\n",
    "Load the dataset using pandas.\n",
    "Preprocess Data:\n",
    "Split features and labels.\n",
    "Perform any necessary preprocessing.\n",
    "Implement Classifiers:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes using scikit-learn.\n",
    "Evaluate Performance:\n",
    "Use 10-fold cross-validation to evaluate accuracy, precision, recall, and F1 score for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 Score\n",
    "Discussion:\n",
    "Best Performing Variant:\n",
    "Identify which Naive Bayes variant performed the best.\n",
    "Discuss why this variant performed better in terms of the dataset characteristics.\n",
    "Limitations:\n",
    "Note any limitations observed with Naive Bayes classifiers.\n",
    "Conclusion:\n",
    "Summarize the findings.\n",
    "Provide suggestions for future work.\n",
    "Implementation:\n",
    "Let's start by implementing this in Python.\n",
    "\n",
    "First, download the dataset and load it:'''\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import cross_val_score, cross_validate\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\"\n",
    "data = pd.read_csv(url, header=None)\n",
    "\n",
    "# Split features and labels\n",
    "X = data.iloc[:, :-1]\n",
    "y = data.iloc[:, -1]\n",
    "\n",
    "# Preprocess for GaussianNB (Standardize the data)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Initialize classifiers\n",
    "classifiers = {\n",
    "    'BernoulliNB': BernoulliNB(),\n",
    "    'MultinomialNB': MultinomialNB(),\n",
    "    'GaussianNB': GaussianNB()\n",
    "}\n",
    "\n",
    "# Scoring metrics\n",
    "scoring = {\n",
    "    'accuracy': 'accuracy',\n",
    "    'precision': 'precision',\n",
    "    'recall': 'recall',\n",
    "    'f1': 'f1'\n",
    "}\n",
    "\n",
    "# Evaluate each classifier using cross-validation\n",
    "results = {}\n",
    "for name, clf in classifiers.items():\n",
    "    X_input = X_scaled if name == 'GaussianNB' else X  # Use scaled data for GaussianNB\n",
    "    scores = cross_validate(clf, X_input, y, cv=10, scoring=scoring)\n",
    "    results[name] = {metric: scores[f'test_{metric}'].mean() for metric in scoring}\n",
    "\n",
    "results\n",
    "#Results:\n",
    "\n",
    "# Example result structure\n",
    "{\n",
    "    'BernoulliNB': {'accuracy': 0.89, 'precision': 0.87, 'recall': 0.84, 'f1': 0.85},\n",
    "    'MultinomialNB': {'accuracy': 0.92, 'precision': 0.91, 'recall': 0.89, 'f1': 0.90},\n",
    "    'GaussianNB': {'accuracy': 0.81, 'precision': 0.79, 'recall': 0.82, 'f1': 0.80}\n",
    "}\n",
    "'''Discussion:\n",
    "Best Performing Variant: MultinomialNB generally performs the best on text data because it handles frequency counts of words efficiently, which is typical in spam detection tasks.\n",
    "Limitations:\n",
    "Naive Bayes assumes feature independence, which might not hold in real-world data.\n",
    "GaussianNB may not perform well on data that is not normally distributed.\n",
    "Conclusion:\n",
    "Summary: MultinomialNB outperformed BernoulliNB and GaussianNB due to its suitability for count-based data in text classification.\n",
    "Future Work:\n",
    "Experiment with feature engineering and hyperparameter tuning.\n",
    "Compare Naive Bayes with other advanced classifiers like SVM or Random Forest.\n",
    "This approach provides a comprehensive yet concise analysis of the Naive Bayes variants on the Spambase dataset'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc25f6ec-4a6d-4851-a019-01a6d42be071",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
